{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sqlite3\n", "import urllib.error\n", "import ssl\n", "from urllib.parse import urljoin\n", "from urllib.parse import urlparse\n", "from urllib.request import urlopen\n", "from bs4 import BeautifulSoup"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ignore SSL certificate errors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ctx = ssl.create_default_context()\n", "ctx.check_hostname = False\n", "ctx.verify_mode = ssl.CERT_NONE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["conn = sqlite3.connect('spider.sqlite')\n", "cur = conn.cursor()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["cur.execute(\nCREATE TABLE IF NOT EXISTS Pages<br>\n", "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,<br>\n", "   error INTEGER, old_rank REAL, new_rank REAL)\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["cur.execute(\nCREATE TABLE IF NOT EXISTS Links<br>\n", "  (from_id INTEGER, to_id INTEGER, UNIQUE(from_id, to_id))\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["cur.execute(\nCREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)\n)<br>\n", "# Check to see if we are already in progress...<br>\n", "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')<br>\n", "row = cur.fetchone()<br>\n", "if row is not None:<br>\n", "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")<br>\n", "else :<br>\n", "    starturl = input('Enter web url or enter: ')<br>\n", "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'<br>\n", "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]<br>\n", "    web = starturl<br>\n", "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :<br>\n", "        pos = starturl.rfind('/')<br>\n", "        web = starturl[:pos]<br>\n", "    if ( len(web) > 1 ) :<br>\n", "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )<br>\n", "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )<br>\n", "        conn.commit()<br>\n", "# Get the current webs<br>\n", "r.execute(\nSELECT url FROM Webs\n)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["webs = list()\n", "for row in cur:\n", "    webs.append(str(row[0]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(webs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["many = 0\n", "while True:\n", "    if ( many < 1 ) :\n", "        sval = input('How many pages:')\n", "        if ( len(sval) < 1 ) : break\n", "        many = int(sval)\n", "    many = many - 1\n", "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n", "    try:\n", "        row = cur.fetchone()\n", "        # print row\n", "        fromid = row[0]\n", "        url = row[1]\n", "    except:\n", "        print('No unretrieved HTML pages found')\n", "        many = 0\n", "        break\n", "    print(fromid, url, end=' ')\n\n", "    # If we are retrieving this page, there should be no links from it\n", "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n", "    try:\n", "        document = urlopen(url, context=ctx)\n", "        html = document.read()\n", "        if document.getcode() != 200 :\n", "            print(\"Error on page: \",document.getcode())\n", "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n", "        if 'text/html' != document.info().get_content_type() :\n", "            print(\"Ignore non text/html page\")\n", "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n", "            conn.commit()\n", "            continue\n", "        print('('+str(len(html))+')', end=' ')\n", "        soup = BeautifulSoup(html, \"html.parser\")\n", "    except KeyboardInterrupt:\n", "        print('')\n", "        print('Program interrupted by user...')\n", "        break\n", "    except:\n", "        print(\"Unable to retrieve or parse page\")\n", "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n", "        conn.commit()\n", "        continue\n", "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n", "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n", "    conn.commit()\n\n", "    # Retrieve all of the anchor tags\n", "    tags = soup('a')\n", "    count = 0\n", "    for tag in tags:\n", "        href = tag.get('href', None)\n", "        if ( href is None ) : continue\n", "        # Resolve relative references like href=\"/contact\"\n", "        up = urlparse(href)\n", "        if ( len(up.scheme) < 1 ) :\n", "            href = urljoin(url, href)\n", "        ipos = href.find('#')\n", "        if ( ipos > 1 ) : href = href[:ipos]\n", "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n", "        if ( href.endswith('/') ) : href = href[:-1]\n", "        # print href\n", "        if ( len(href) < 1 ) : continue"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t# Check if the URL is in any of the webs\n", "        found = False\n", "        for web in webs:\n", "            if ( href.startswith(web) ) :\n", "                found = True\n", "                break\n", "        if not found : continue\n", "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n", "        count = count + 1\n", "        conn.commit()\n", "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n", "        try:\n", "            row = cur.fetchone()\n", "            toid = row[0]\n", "        except:\n", "            print('Could not retrieve id')\n", "            continue\n", "        # print fromid, toid\n", "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    print(count)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cur.close()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}